---
phase: 21-hourly-data-integration
plan: 01
type: execute
---

<objective>
Add hourly OHLC data fetch from CoinGecko with caching to enable multi-timeframe divergence detection.

Purpose: Foundation for 6-timeframe divergence analysis (1h, 4h, 12h candles aggregated from hourly data)
Output: Working hourly data pipeline with caching, integrated into main data flow
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/2025-01-25-multi-timeframe-divergence-display.md

# Key source files
@src/coingecko.py - Current API client with daily endpoint
@src/data_store.py - Current caching pattern
@app.py - Main data fetch flow in fetch_all_data()

# Prior decisions
- CoinGecko market_chart endpoint returns hourly data for 2-90 day ranges when interval param is omitted
- Current code uses interval="daily" which forces daily granularity
- Hourly data is ~24x larger than daily, needs separate cache with TTL

# Technical approach from spec
- Hourly data (90 days) for 1h, 4h, 12h candle aggregation
- API impact: 2 calls per coin (daily + hourly) vs current 1 call
- ~96 calls total for 48 coins per refresh (within CoinGecko Pro limits)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add hourly endpoint to CoinGecko client</name>
  <files>src/coingecko.py</files>
  <action>
Add two new methods to CoinGeckoClient:

1. `get_coin_market_chart_hourly(coin_id, vs_currency="usd", days=90)`:
   - Same as `get_coin_market_chart` but WITHOUT the `interval: "daily"` param
   - CoinGecko auto-returns hourly granularity for 2-90 day ranges
   - Returns dict with 'prices', 'market_caps', 'total_volumes' arrays at hourly resolution

2. `get_coins_hourly_history(coin_ids, vs_currency="usd", days=90)`:
   - Batch fetch using asyncio.gather like existing `get_coins_history`
   - Returns dict mapping coin_id -> hourly market_chart response
   - Graceful error handling (exclude failed coins from result)

Keep existing daily methods unchanged - they're used elsewhere.
  </action>
  <verify>
Add to existing coingecko.py and verify syntax:
```bash
python -c "from src.coingecko import CoinGeckoClient; print('Import OK')"
```
  </verify>
  <done>Both hourly methods exist, import works, type hints present</done>
</task>

<task type="auto">
  <name>Task 2: Add hourly data caching</name>
  <files>src/data_store.py</files>
  <action>
Add separate cache for hourly data with TTL:

1. New constants:
   - `HOURLY_DATA_FILE = DATA_DIR / "hourly_data.json"`
   - `HOURLY_CACHE_TTL_MINUTES = 60` (1 hour TTL - hourly data doesn't need frequent refresh)

2. New functions:

`save_hourly_data(hourly_history: dict[str, dict], last_updated: datetime) -> None`:
   - Saves raw hourly history data (coin_id -> market_chart response)
   - Include last_updated timestamp for TTL checking

`load_hourly_data() -> dict | None`:
   - Returns dict with 'hourly_history' and 'last_updated' keys
   - Returns None if file doesn't exist or JSON is corrupted

`is_hourly_cache_valid() -> bool`:
   - Check if cache exists and is within TTL
   - Return False if expired or missing

Keep existing functions unchanged - they handle computed results (coin_data, divergence_data).
  </action>
  <verify>
```bash
python -c "from src.data_store import save_hourly_data, load_hourly_data, is_hourly_cache_valid; print('Import OK')"
```
  </verify>
  <done>Three new functions exist, import works, TTL constant defined as 60 minutes</done>
</task>

<task type="auto">
  <name>Task 3: Integrate hourly fetch into main data flow</name>
  <files>app.py</files>
  <action>
Modify `fetch_all_data()` to also fetch hourly data:

1. Add import for new hourly cache functions at top of file

2. In `fetch_all_data()`, after the existing daily history fetch:
   - Check if hourly cache is valid using `is_hourly_cache_valid()`
   - If valid: load from cache with `load_hourly_data()`
   - If not valid: fetch hourly with `client.get_coins_hourly_history(coin_ids, days=90)`
   - Save to cache with `save_hourly_data()`

3. Return the hourly_history as part of the tuple return value:
   - Current: `return result, divergence_result, failed_count, btc_regime, btc_weekly_rsi`
   - New: `return result, divergence_result, failed_count, btc_regime, btc_weekly_rsi, hourly_history`

4. Update all callers of `fetch_all_data()` to handle the new return value:
   - The refresh button handler around line 1200
   - Store `hourly_history` in `st.session_state.hourly_history`

5. Add `hourly_history` to session state initialization:
   - Add `st.session_state.hourly_history = None` in the initialization block

Note: Phase 22 will use `st.session_state.hourly_history` for multi-timeframe RSI/divergence calculations. For now, just fetch and store it.
  </action>
  <verify>
```bash
python -c "import app; print('Import OK')"
```
Manual: Run `streamlit run app.py`, click Refresh, check data/hourly_data.json exists
  </verify>
  <done>
- Hourly data fetched on refresh (with cache check)
- hourly_data.json created in data/ directory
- st.session_state.hourly_history populated
- No errors in console
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.coingecko import CoinGeckoClient"` - no errors
- [ ] `python -c "from src.data_store import save_hourly_data, load_hourly_data, is_hourly_cache_valid"` - no errors
- [ ] `python -c "import app"` - no errors
- [ ] Run app, click Refresh, verify data/hourly_data.json is created
- [ ] Verify hourly_data.json contains hourly timestamps (e.g., multiple entries within a single day)
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Hourly data pipeline working end-to-end
- Cache TTL logic functioning (second refresh within 1 hour uses cache)
- No regressions in existing functionality
</success_criteria>

<output>
After completion, create `.planning/phases/21-hourly-data-integration/21-01-SUMMARY.md`
</output>
